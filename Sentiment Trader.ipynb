{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#author: James Chan Â© 2018\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import st_utils as ut\n",
    "import datetime as dt\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from dateutil import parser\n",
    "from datetime import timedelta, date\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize date range\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2018-8-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'ok', 'totalResults': 1798, 'articles': []}\n",
      "total number of articles  1798\n",
      "downloading page:  1\n",
      "downloading page:  2\n",
      "downloading page:  3\n",
      "downloading page:  4\n",
      "downloading page:  5\n",
      "downloading page:  6\n",
      "downloading page:  7\n",
      "downloading page:  8\n",
      "downloading page:  9\n",
      "downloading page:  10\n",
      "downloading page:  11\n",
      "downloading page:  12\n",
      "downloading page:  13\n",
      "downloading page:  14\n",
      "downloading page:  15\n",
      "downloading page:  16\n",
      "downloading page:  17\n",
      "downloading page:  18\n",
      "download complete\n"
     ]
    }
   ],
   "source": [
    "# grab news.  nnly need to do this once\n",
    "keywords = ['tesla'] #these three stocks move largely base on news\n",
    "news_source = 'wsj.com, bloomberg.com, cnbc.com'\n",
    "df = ut.get_news(keywords, start_date, end_date, news_source)\n",
    "df.to_csv('news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get Tesla's prices\n",
    "dates = pd.date_range(start_date, end_date)\n",
    "file = 'TSLA.csv'\n",
    "df_stock = ut.get_data(file, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate return \n",
    "df_stock['Return'] = df_stock['Adj Close'].pct_change()\n",
    "df_stock.drop(df_stock.index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_news = pd.read_csv('news_dataset.csv', index_col=0, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map published time to close date.  see figure.1\n",
    "def map_to_close_date(published_date):\n",
    "    dt = parser.parse(published_date[:-1]) #-1 to ignore the Z, which is GMT.\n",
    "    dt = dt - timedelta(hours=20) #shift back by 20 hrs.\n",
    "    return pd.Timestamp(year=dt.year, month=dt.month, day=dt.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map news to close date\n",
    "df_news['Published'] = df_news['Published'].apply(map_to_close_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine title and body into single text\n",
    "df_news['Text'] = df_news['Title'] + ' ' + df_news['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just keep published, and the combined text\n",
    "df_news = df_news[['Published','Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge news and stock\n",
    "df_stock['Published'] = df_stock.index\n",
    "df_merged = pd.merge(df_news, df_stock, how='left', on='Published')\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged.sort_values(by='Published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock['Published'] = df_stock.index\n",
    "df_merged = df_merged.dropna()\n",
    "df_merged.sort_values(by='Published')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_news(text):\n",
    "    if(type(text)==float):\n",
    "        return []\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    stmr = PorterStemmer()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = (text.translate(translator))\n",
    "    text = \"\".join(c for c in text if ord(c)<128) #strip no n ascii characters\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    t = []\n",
    "    minlen =  4\n",
    "    maxlen = 20 \n",
    "    for token in tokens:\n",
    "        if len(token) < minlen or len(token) > maxlen or token.isnumeric() or token in stopwords.words('english'):\n",
    "            pass\n",
    "        else:\n",
    "            token = lmtzr.lemmatize(token)\n",
    "            token = stmr.stem(token)\n",
    "            t.append(token)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Text'] = df_merged['Text'].apply(tokenize_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#construct vocabulary and frequency.  frequency is not really needed, just nice to have for understanding the concept.\n",
    "vocab = {}\n",
    "frequency = {}\n",
    "ignore = ['bloomberg', 'journal']\n",
    "index = 0\n",
    "for title in df_merged['Text']:\n",
    "    for word in title:\n",
    "        if word in ignore:\n",
    "            continue\n",
    "        if word not in vocab:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "            frequency[word] = 1\n",
    "        else:\n",
    "            frequency[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize words frequency\n",
    "for i in sorted(frequency.items(), key=lambda x:x[1], reverse=True):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot(text):\n",
    "    #return one hot vector of shape (vocab_len, 1)\n",
    "    array = np.zeros((vocab_len, 1))\n",
    "    for word in text:\n",
    "        if word in vocab:\n",
    "            index = vocab[word]\n",
    "            array[index] = 1\n",
    "    return array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the exciting part. here we are going to create the training data!\n",
    "X = np.empty((vocab_len,0))\n",
    "for text in df_merged['Text']:\n",
    "    array = to_one_hot(text)\n",
    "    X = np.hstack((X,array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "Y = df_merged['Return'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4535, 1227)\n",
      "(1227,)\n"
     ]
    }
   ],
   "source": [
    "#verify shape\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False,  True,  True],\n",
       "       [ True,  True,  True, ...,  True, False, False]])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = .001\n",
    "negative_pos = (Y < -threshold)\n",
    "positive_pos = (Y > threshold)\n",
    "q = np.vstack((Y > -threshold, Y < threshold))\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=3, init='uniform', activation='relu'))\n",
    "model.add(Dense(20, input_dim=3, init='uniform', activation='relu'))\n",
    "model.add(Dense(20, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
